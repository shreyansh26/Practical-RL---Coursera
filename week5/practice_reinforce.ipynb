{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"practice_reinforce.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce.ipynb","timestamp":1602138443029}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9AeMIo2v6SlY"},"source":["# REINFORCE in TensorFlow\n","\n","Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"rg3cvHIG6Sla","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1602137995327,"user_tz":-330,"elapsed":1274,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"dd6cfa28-3b58-409f-e30f-6761b3ef0ddf"},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    %tensorflow_version 1.x\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CuUdPr2A6Slg"},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bz8f89uF6Sll"},"source":["A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."]},{"cell_type":"code","metadata":{"id":"CrZML0oe6Sll","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1602137996695,"user_tz":-330,"elapsed":2630,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"8438b0e7-59df-4897-c8b6-501ef207f346"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fe576f0fb38>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"uqphPCyz6Slo"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"ijpYQOPm6Slp"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"_guL2R9T6Slp"},"source":["import tensorflow as tf\n","\n","sess = tf.InteractiveSession()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ze-ThM7X6Slr"},"source":["# create input variables. We only need <s, a, r> for REINFORCE\n","ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n","ph_actions = tf.placeholder('int32', name=\"action_ids\")\n","ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yt9_CAPH6Slt","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1602137999752,"user_tz":-330,"elapsed":5675,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"7e931963-3b91-4f60-cf28-b30473ab0bb6"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","network = Sequential()\n","network.add(Dense(32, activation='relu', input_shape=state_dim))\n","network.add(Dense(32, activation='relu'))\n","network.add(Dense(n_actions, activation='linear'))\n","\n","logits = network(ph_states)\n","\n","policy = tf.nn.softmax(logits)\n","log_policy = tf.nn.log_softmax(logits)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2dn2S4lK6Slu"},"source":["# Initialize model parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oY-tCrZ6Slv"},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    return policy.eval({ph_states: [states]})[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-R1akHf46Slw"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"rLTlaOlr6Slx"},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(s)\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice(n_actions, 1, p=action_probs)[0]\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RqoIOKq6Sly"},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qmC6bfyG6Slz"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"GLW-VTOd6Slz"},"source":["from collections import deque\n","def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    cumulative_rewards = deque([rewards[-1]])\n","\n","    for i in range(len(rewards)-2, -1, -1):\n","      cumulative_rewards.appendleft(rewards[i]+gamma*cumulative_rewards[0])\n","\n","    return cumulative_rewards"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDZHyBJe6Sl1","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1602137999753,"user_tz":-330,"elapsed":5657,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1f9c45d5-d578-406e-f097-53d4b91a6460"},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2zKysXZt6Sl2"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"obZA1OLO6Sl2"},"source":["# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n","log_policy_for_actions = tf.gather_nd(log_policy, indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPSC_Kb56Sl3"},"source":["# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n","# You may use log_policy_for_actions to get log probabilities for actions taken.\n","# Also recall that we defined ph_cumulative_rewards earlier.\n","\n","J = tf.reduce_mean(log_policy_for_actions*ph_cumulative_rewards)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7arLccRw6Sl4"},"source":["As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n","\n","$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"]},{"cell_type":"code","metadata":{"id":"paUog-Re6Sl4"},"source":["# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n","# being deterministic, harming exploration.\n","\n","entropy = -tf.reduce_sum(policy * log_policy, 1, name=\"entropy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33xWiFXf6Sl5"},"source":["# # Maximizing X is the same as minimizing -X, hence the sign.\n","loss = -(J + 0.1 * entropy)\n","\n","update = tf.train.AdamOptimizer().minimize(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73WOJVm86Sl6"},"source":["def train_on_session(states, actions, rewards, t_max=1000):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    cumulative_rewards = get_cumulative_rewards(rewards)\n","    update.run({\n","        ph_states: states,\n","        ph_actions: actions,\n","        ph_cumulative_rewards: cumulative_rewards,\n","    })\n","    return sum(rewards)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6rpWlgA36Sl8"},"source":["# Initialize optimizer parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1RGHBTj6Sl9"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"kav_Ijjl6Sl9","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1602138084876,"user_tz":-330,"elapsed":90758,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"e781f7ad-9bd4-4533-bbbe-1f1561c79d4b"},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","\n","    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean reward: 30.630\n","mean reward: 56.870\n","mean reward: 141.280\n","mean reward: 273.970\n","mean reward: 289.360\n","mean reward: 255.990\n","mean reward: 522.600\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V8-hC4gT6Sl-"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"n2Y-2YPu6Sl-"},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OG2NHdv6Sl_","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.2244.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1602138108398,"user_tz":-330,"elapsed":114271,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1e8dab9f-609a-4ad8-dae6-16682688b89f"},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.2244.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"wOjbcsam6SmA"},"source":["# from submit import submit_cartpole\n","# submit_cartpole(generate_session, '', '')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FsErV9Kt6SmB"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}